{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTSA 5510 - Unsupervised Algorithms in Machine Learning Final Project\n",
    "Chris Murphy 07/23/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my final project, I wanted to attempt to create a sentiment analysis model. As I was thinking about ideas for my project, I thought back to the BBC News Classification mini project we were tasked with completing in week 4 and was thinking about different areas of unsupervised machine learning algorithms that could be applied to the same dataset. In this project, the articles were classified into five distinct categories but what if a model could be bilt to understand if the document that was being analyzed was a positive story or a negative story? This was an interesting project idea that I wanted to learn more about. After some additional research, I found there are many methodogies to answer this same question in the broader sentiment analysis category.      \n",
    "\n",
    "With my mind made up on undertaking an in depth sentiment analysis project, I needed some data to work with. I had found a few interesting datasets that contained scraped Twitter data on various topics, however, nothing really resonated with me. After thinking about the data I could use some more, I realized I had all the data I needed to copmlete a sentiment analysis project. In my day job as a Data Analyst for an e-commerce company, everday I'm working with transactional Amazon data. Sure enough, there was Amazon review data that I could query. This data would become my foundation for my final project.\n",
    "\n",
    "The dataset contains 10,000 distinct reviews for users who had purchased items in various categories in the United States. A sample of the data can be viewed below. Here are the columns and a basic overview of what they indicate:\n",
    "\n",
    "- ID: The unique identifier of the review\n",
    "- REVIEW_DATE: The date when the review was left\n",
    "- IS_VERIFIED: Whether or not the review was verified. (I did not end up using this column)\n",
    "- RATING: The rating the user gave for the product on a 1 to 5 scale\n",
    "- REVIEW_TITLE: The title of the review \n",
    "- REVIEW_TEXT: The review the user left for the product \n",
    "- NAME: The country the user is in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn specific imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import\n",
    "reviews = pd.read_csv('data/reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is an initial view of a sample of rows in the dataset\n",
    "reviews.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the sample dataframe above, the 'NAME' field is misleading, it will be changed from 'NAME' to 'COUNTRY'\n",
    "reviews.rename(columns={'NAME': 'COUNTRY'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 10,000 reviews in our initial dataset.\n",
    "# The 'REVIEW_TITLE' column has one null entry which we will remove in a subsequent cell\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the only row with a null value. Since we really care about the 'REVIEW_TEXT'\n",
    "# this row could probably be left in but I will remove it just to be safe\n",
    "reviews[reviews['REVIEW_TITLE'].isnull()]\n",
    "reviews.dropna(subset = ['REVIEW_TITLE'], axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double checking that there are no more null values in the updated dataset\n",
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two more adjustments that need to be made. The 'REVIEW_DATE' column should be a datetime\n",
    "# also the 'ID' field should be an object\n",
    "reviews['REVIEW_DATE'] = pd.to_datetime(reviews['REVIEW_DATE'])\n",
    "reviews['ID'] = reviews['ID'].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing non-english reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another issue that will impact our results later on in the model testing phase. Even though these reviews are sourced from Amazon.com, there are reviews that are in different languages. A mask will be created to filter our all non English reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The langdetect package can be used to detect the language of a body of text.\n",
    "# the package can then be used in a basic function to return the particular language of the review.\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        lang = 'unknown'\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the 'REVIEW_TEXT' column of our dataset through the detect_language function from above\n",
    "reviews['LANGUAGE'] = reviews['REVIEW_TEXT'].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the results of the  we can see that there are many english reviews,\n",
    "# but also other language as determined by the langdetect package\n",
    "reviews['LANGUAGE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a portuguese review in the dataset.\n",
    "# This review is an example of a review that will be removed \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "reviews['REVIEW_TEXT'][reviews['LANGUAGE'] == 'pt'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# although it should be said that this package is not perfect.\n",
    "# for example, this review was classified as 'Norwegian' but in reality it is english.\n",
    "# instead of going through each example, I decided to leave these edge cases alone\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "reviews['REVIEW_TEXT'][reviews['LANGUAGE'] == 'no'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a mask to get only english reviews\n",
    "english_mask = reviews['LANGUAGE'] == 'en'\n",
    "english_reviews = reviews[english_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the word count of the view. This will be interesting to visualize later on in the notebook\n",
    "english_reviews['word_count'] = english_reviews['REVIEW_TEXT'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the 'REVIEW_DATE' column, getting the month and year that the review was submitted.\n",
    "# this will be interesting to visualize\n",
    "english_reviews['YEAR'] = english_reviews['REVIEW_DATE'].dt.year\n",
    "english_reviews['MONTH'] = english_reviews['REVIEW_DATE'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of rows in the final dataset is: {len(english_reviews)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This first visualization looks at the count of reviews based on their \n",
    "plt.style.use('fivethirtyeight')\n",
    "fix, ax = plt.subplots(figsize = (25, 10))\n",
    "\n",
    "ratings = reviews['RATING'].value_counts()\n",
    "ind = reviews['RATING'].unique()\n",
    "\n",
    "plt.bar(ind, ratings, color = 'firebrick', align = 'center')\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.text(p.get_x() + p.get_width() / 2,\n",
    "            p.get_height(),\n",
    "            '{:.0f}'.format(p.get_height()),\n",
    "            ha='center',\n",
    "            va='bottom'\n",
    "           )\n",
    "\n",
    "plt.title('Count of Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is unbalanced, but that is ok in our context as we will not be exploring the relationship between variables, just the underlying text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots(figsize = (25, 10))\n",
    "\n",
    "sns.histplot(data=english_reviews, x='word_count', hue='RATING', multiple='dodge', legend='RATING', kde = True, bins = 50)\n",
    "\n",
    "plt.xlabel('Word count per review')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Number of words by review histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots(figsize = (25, 10))\n",
    "\n",
    "wc_group = english_reviews.groupby('RATING')['word_count'].apply(list)\n",
    "data = [wc_group[cat] for cat in english_reviews['RATING'].unique()]\n",
    "\n",
    "bp = plt.boxplot(data, labels = english_reviews['RATING'].unique(), patch_artist=True)\n",
    "\n",
    "colors = ['blue', 'black', 'green', 'yellow', 'purple']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "plt.title('Word count boxplot')\n",
    "plt.xlabel('Review score')\n",
    "plt.ylabel('Word count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_avg_rating = english_reviews.groupby(['YEAR', 'MONTH'])['RATING'].mean().reset_index()\n",
    "filtered_rating = monthly_avg_rating[monthly_avg_rating['YEAR'].isin([2021, 2022, 2023, 2024])]\n",
    "pivot_df = filtered_rating.pivot(index = 'MONTH', columns = 'YEAR', values='RATING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots(figsize = (25, 10))\n",
    "\n",
    "for col in pivot_df.columns:\n",
    "    plt.plot(pivot_df.index, pivot_df[col], marker = 'o', label = col)\n",
    "\n",
    "plt.xlabel('month')\n",
    "plt.ylabel('Avg. Rating')\n",
    "plt.title('Avg. rating over time')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "english_reviews['REVIEW_TEXT'].sample(1, random_state = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    final_text = []\n",
    "    for row in text:\n",
    "        clean_row = \"\".join(u for u in row if u not in ('?', '.', ';', ':', '!', '\"', \"'\", '(', ')', '[', ']', '/', ',', '-', '*'))\n",
    "        final_text.append(clean_row.lower())\n",
    "    \n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = RegexpTokenizer(f'[a-zA-Z0-9]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reviews['REVIEW_TEXT'] = remove_punctuation(english_reviews['REVIEW_TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words = 'english', ngram_range = (1,2), tokenizer = token.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_counts = cv.fit_transform(english_reviews['REVIEW_TEXT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, english_reviews['RATING'], test_size = 0.2, random_state = 42, stratify=english_reviews['RATING'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the MNB model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = MNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = accuracy_score(predicted, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The accuracy score of the Multimodial Naive Bayes model is: {round(acc_score, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(Y_test, predicted)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = np.unique(Y_test))\n",
    "disp.plot(cmap = plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polartiy determines the sentiment of the text. -1 = highly negative review and 1 denotes a highly positive sentiment.\n",
    "\n",
    "Subjectivity determines whether a text input is fact vs personal information. 0 denotes a fact and 1 denotes a personal opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reviews['REVIEW_TEXT'].iloc[284]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = TextBlob(english_reviews['REVIEW_TEXT'].iloc[284]).sentiment.polarity\n",
    "s1 = TextBlob(english_reviews['REVIEW_TEXT'].iloc[284]).sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The polarity of the test token is : {round(p1, 4)} and the subjectivity of the test token is: {round(s1, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The actual rating was: {english_reviews['RATING'].iloc[284]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_polarity(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "def calculate_subjectivity(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reviews['TEXT_BLOB_POLARITY_RAW'] = english_reviews['REVIEW_TEXT'].apply(calculate_polarity)\n",
    "english_reviews['TEXT_BLOB_SUBJECTIVITY_RAW'] = english_reviews['REVIEW_TEXT'].apply(calculate_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_rating(value):\n",
    "    mapped_value = (value + 1) * 2.5 + 0.5\n",
    "    mapped_rating = round(mapped_value)\n",
    "    mapped_rating = max(1, min(5, mapped_rating))\n",
    "    return mapped_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reviews['TEXT_BLOB_POLARITY_MAPPED'] = english_reviews['TEXT_BLOB_POLARITY_RAW'].apply(map_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reviews.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reviews['TEXT_BLOB_POLARITY_MAPPED'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reviews['RATING'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score_2 = accuracy_score(english_reviews['TEXT_BLOB_POLARITY_MAPPED'].values, english_reviews['RATING'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The accuracy score of the \\'Text Blob\\' model accuracy score is: {round(acc_score_2, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(english_reviews['RATING'].values, english_reviews['TEXT_BLOB_POLARITY_MAPPED'].values)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = np.unique(Y_test))\n",
    "disp.plot(cmap = plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1 = english_reviews['REVIEW_TEXT'].iloc[1336]\n",
    "t_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = sentiment.polarity_scores(t_1)\n",
    "print(f'The VADER sentiment of review 1337 is: {sent_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The actual rating for review 1337 is: {english_reviews['RATING'].iloc[1336]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment(text):\n",
    "    vader = sentiment.polarity_scores(text)\n",
    "    return vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reviews['VADER_SENTIMENT_RAW'] = english_reviews['REVIEW_TEXT'].apply(calculate_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reviews.iloc[72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sentiment(sentiment):\n",
    "    pos_score = sentiment['pos']\n",
    "    neg_score = sentiment['neg']\n",
    "\n",
    "    weighted_score = (pos_score - neg_score) * 2 + 3\n",
    "    mapped_score = round(max(1, min(5, weighted_score)))\n",
    "    return mapped_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The mapped sentiment for review 1337 is: {map_sentiment(sent_1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reviews['VADER_SENTIMENT_MAPPED'] = english_reviews['VADER_SENTIMENT_RAW'].apply(map_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reviews.iloc[72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score_3 = accuracy_score(english_reviews['VADER_SENTIMENT_MAPPED'].values, english_reviews['RATING'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The accuracy score of the \\'VADER\\' model accuracy score is: {round(acc_score_3, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(english_reviews['RATING'].values, english_reviews['VADER_SENTIMENT_MAPPED'].values)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = np.unique(Y_test))\n",
    "disp.plot(cmap = plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclsion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

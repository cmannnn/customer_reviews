{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTSA 5510 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from itertools import permutations\n",
    "\n",
    "# old imports\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# latest imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import\n",
    "reviews = pd.read_csv('data/reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is an initial view of a sample of rows in the dataset\n",
    "reviews.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the sample dataframe above, the 'NAME' field is misleading, it will be changed from 'NAME' to 'COUNTRY'\n",
    "reviews.rename(columns={'NAME': 'COUNTRY'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 10,000 reviews in our initial dataset.\n",
    "# The 'REVIEW_TITLE' column has one null entry which we will remove in a subsequent cell\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the only row with a null value. Since we really care about the 'REVIEW_TEXT'\n",
    "# this row could probably be left in but I will remove it just to be safe\n",
    "reviews[reviews['REVIEW_TITLE'].isnull()]\n",
    "reviews.dropna(subset = ['REVIEW_TITLE'], axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double checking that there are no more null values in the updated dataset\n",
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two more adjustments that need to be made. The 'REVIEW_DATE' column should be a datetime\n",
    "# also the 'ID' field should be an object\n",
    "reviews['REVIEW_DATE'] = pd.to_datetime(reviews['REVIEW_DATE'])\n",
    "reviews['ID'] = reviews['ID'].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing non-english reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another issue that will impact our results later on in the model testing phase. Even though these reviews are sourced from Amazon.com, there are reviews that are in different languages. A mask will be created to filter our all non English reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The langdetect package can be used to detect the language of a body of text.\n",
    "# the package can then be used in a basic function to return the particular language of the review.\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        lang = 'unknown'\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the 'REVIEW_TEXT' column of our dataset through the detect_language function from above\n",
    "reviews['LANGUAGE'] = reviews['REVIEW_TEXT'].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the results of the  we can see that there are \n",
    "reviews['LANGUAGE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a portuguese review in the dataset\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "reviews['REVIEW_TEXT'][reviews['LANGUAGE'] == 'pt'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_mask = reviews['LANGUAGE'] == 'en'\n",
    "english_reviews = reviews[english_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reviews['word_count'] = english_reviews['REVIEW_TEXT'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fix, ax = plt.subplots(figsize = (25, 10))\n",
    "\n",
    "ratings = reviews['RATING'].value_counts()\n",
    "ind = reviews['RATING'].unique()\n",
    "\n",
    "plt.bar(ind, ratings, color = 'firebrick', align = 'center')\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.text(p.get_x() + p.get_width() / 2,\n",
    "            p.get_height(),\n",
    "            '{:.0f}'.format(p.get_height()),\n",
    "            ha='center',\n",
    "            va='bottom'\n",
    "           )\n",
    "\n",
    "plt.title('Count of Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is unbalanced, but that is ok in our context as we will not be exploring the relationship between variables, just the underlying text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots(figsize = (25, 10))\n",
    "\n",
    "sns.histplot(data=english_reviews, x='word_count', hue='RATING', multiple='dodge', legend='RATING', kde = True, bins = 50)\n",
    "\n",
    "plt.xlabel('Word count per review')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Number of words by review histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots(figsize = (25, 10))\n",
    "\n",
    "wc_group = english_reviews.groupby('RATING')['word_count'].apply(list)\n",
    "data = [wc_group[cat] for cat in english_reviews['RATING'].unique()]\n",
    "\n",
    "bp = plt.boxplot(data, labels = english_reviews['RATING'].unique(), patch_artist=True)\n",
    "\n",
    "colors = ['blue', 'black', 'green', 'yellow', 'purple']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "plt.title('Word count boxplot')\n",
    "plt.xlabel('Review score')\n",
    "plt.ylabel('Word count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## graph of average score over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "english_reviews['REVIEW_TEXT'].sample(1, random_state = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    final = \"\".join(u for u in text if u not in ('?', '.', ';', ':', '!', '\"', \"'\", '(', ')', '[', ']', '/', ',', '-'))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = RegexpTokenizer(f'[a-zA-Z0-9]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token = 'This scanner is super easy to use and scans photos very quickly/accurately. The software comes on a CD, which I couldnâ€™t use since I was using my laptop but the software is easy to find online anyway. The one thing to note is you will need a USB port to connect scanner to laptop. A lot of laptops are USBc only these days so that might be tough. Otherwise, a great product.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token.tokenize(remove_punctuation(test_token))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reviews['REVIEW_TEXT'] = remove_punctuation(english_reviews['REVIEW_TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words = 'english', ngram_range = (1,1), tokenizer = token.tokenize, max_features = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_counts = cv.fit_transform(english_reviews['REVIEW_TEXT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, english_reviews['REVIEW_TEXT'], test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = MNB.predict(X_test)\n",
    "accuracy_score = accuracy_score(predicted, Y_test)\n",
    "print(f'The accuracy score of the Multimodial Naive Bayes model is: {accuracy_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final sample of the english_reviews dataset that will be split into training and testing datasets\n",
    "english_reviews.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reviews_train, english_reviews_test = train_test_split(english_reviews, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of rows in the training dataset is: {len(english_reviews_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of rows in the testing dataset is: {len(english_reviews_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = english_reviews_train['RATING'].tolist()\n",
    "english_reviews_train_text = english_reviews_train['REVIEW_TEXT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range =(1,2), stop_words = 'english', max_features = 1000)\n",
    "features = tfidf.fit_transform(english_reviews_train_text)\n",
    "df_tfidf = pd.DataFrame(features.toarray(), columns = tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original review\n",
    "english_reviews_train.iloc[2]['REVIEW_TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_three  = df_tfidf.iloc[2]\n",
    "sorted_doc_three_scores = df_tfidf_three.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_doc_three_scores[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots(figsize = (25, 10))\n",
    "\n",
    "sorted_doc_three_scores[:20].plot(kind = 'bar')\n",
    "plt.title('TF-IDF score for review 3')\n",
    "plt.ylabel('TF-IDF score')\n",
    "plt.xlabel('Terms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nmf = NMF( init = 'random', beta_loss = 'frobenius',  verbose = 1, random_state = 42).fit(df_tfidf)\n",
    "# max_iter = 100,\n",
    "# n_components = 5,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model_nmf.transform(df_tfidf)\n",
    "pd.DataFrame(weights).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(weights, axis = 1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions are zero indexed, need to add one to match the y values\n",
    "pred_adjusted = pred + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_compare(y, yp):\n",
    "    '''Function that for all permutations of labels, retruns the best accuracy score and order'''\n",
    "    global best_ordering\n",
    "    unique_labels = np.unique(yp)\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for order in permutations(unique_labels):\n",
    "        label_map = {unique_labels[i]: order[i] for i in range(len(unique_labels))}\n",
    "        reordered_yp = np.vectorize(label_map.get)(yp)\n",
    "\n",
    "        current_score = accuracy_score(y, reordered_yp)\n",
    "        if current_score > best_accuracy:\n",
    "            best_accuracy = current_score\n",
    "            best_ordering = order\n",
    "        \n",
    "    return best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The best accuracy achieved with random hyperparameters for the NMF model is: {label_compare(y, pred_adjusted)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.unique(pred_adjusted)\n",
    "label_map = dict(zip(labels, best_ordering))\n",
    "reorder_yp = np.array([label_map[lbl] for lbl in pred_adjusted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reorder_yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = english_reviews_train['RATING'].unique().tolist()\n",
    "l = pd.Index(label_names)\n",
    "cm = pd.crosstab(y, reorder_yp)\n",
    "# cm = pd.crosstab(l[y], l[reorder_yp])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(cm, annot=True, square=True, cmap='Reds', fmt='.0f', cbar=False)\n",
    "plt.title('NMF random model confusion matrix', fontsize = 12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xticks(rotation=20, fontsize=8)\n",
    "plt.yticks(fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scoring(estimator, X, y):\n",
    "    'Creating a scoring function to be used in our GridSearchCV'\n",
    "    W = estimator.transform(X)\n",
    "    predictions = np.argmax(W, axis=1)\n",
    "    return label_compare(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfidfvectorizer', TfidfVectorizer(ngram_range =(1,2), stop_words = 'english', max_features = 1000)),\n",
    "    ( 'nmf', NMF(n_components = 5, init = 'random', beta_loss = 'frobenius'))\n",
    "])\n",
    "\n",
    "params = [{\n",
    "    # 'tfidfvectorizer__ngram_range': [(1, 2)],\n",
    "    'tfidfvectorizer__min_df': [1, 2, 3],\n",
    "    'nmf__alpha_W': [0, .01, .02],\n",
    "    'nmf__alpha_H': [0, .01, .02]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, param_grid = params, scoring = custom_scoring, cv = 10)\n",
    "x = english_reviews_train['REVIEW_TEXT']\n",
    "y = english_reviews_train['RATING']\n",
    "grid.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(grid.cv_results_).sort_values(by='mean_test_score')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "cv_results[['mean_fit_time', 'params', 'mean_test_score', 'rank_test_score']].sort_values(by='rank_test_score').iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
